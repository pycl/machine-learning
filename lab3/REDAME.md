## Теория алгоритм
### KNN 
- **Евклидово расстояние (Euclidean Distance)**：

$
d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
$

- **Манхэттенское расстояние (Manhattan Distance)**：

$
d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^n |x_i - y_i|
$

- **Расстояние Минковского (Minkowski Distance)**：

$
d(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{\frac{1}{p}}
$

при $ p = 2 $ это евклидово расстояние, а при $ p = 1 $ — манхэттенское расстояние.

После выбора $ K $ ближайших соседей, классификация выполняется следующим образом:

**Метод большинства голосов** weights='uniform': выбирается класс, который имеет большинство среди $ K $ соседей, как результат прогноза.

$
\hat{y} = \arg \max_{c \in C} \sum_{i=1}^{K} \mathbb{I}(y_i = c)
$

где $ \mathbb{I} $ — это индикаторная функция, $ C $ — множество классов.

**Метод взвешенного голосования** weights='distance': веса голосов соседей корректируются в зависимости от их расстояния; чем ближе сосед, тем больше его вес.

### GaussianNaiveBayes
#### Формула Байеса

$$
P(C|X) = \frac{P(X|C)P(C)}{P(X)}
$$

Где:

- $P(C|X)$ — **апостериорная вероятность**, то есть вероятность принадлежности к классу $C$ при наличии признака $X$.
- $P(X|C)$ — **вероятность правдоподобия**, то есть вероятность наблюдения признака $X$ при условии, что объект принадлежит к классу $C$.
- $P(C)$ — **априорная вероятность**, то есть априорная вероятность класса $C$.
- $P(X)$ — **маргинальная вероятность**, то есть общая вероятность наблюдения признака $X$.
  
  Наивный байесовский классификатор предполагает, что все признаки являются взаимно независимыми при заданном классе. Следовательно, совместная вероятность правдоподобия может быть разложена в произведение вероятностей отдельных признаков:<br>

$
P(X|C) = \prod_{i=1}^{n} P(x_i|C)
$

где $ X = (x_1, x_2, \dots, x_n)$ — вектор признаков, $n$ — количество признаков.
Гауссовский наивный байесовский классификатор далее предполагает, что каждый признак при заданном классе следует гауссовскому распределению. Таким образом, вероятность правдоподобия $P(x_i|C)$ может быть представлена как:

$
P(x_i|C) = \frac{1}{\sqrt{2\pi\sigma_C^2}} \exp \left( -\frac{(x_i - \mu_C)^2}{2\sigma_C^2} \right)
$

$
\log P(x_i|C) = -\frac{1}{2} \log(2\pi\sigma_C^2) - \frac{(x_i - \mu_C)^2}{2\sigma_C^2}
$


где:

- $ \mu_C $ — среднее значение признака при классе $C$.
- $ \sigma_C^2 $ — дисперсия признака при классе $C$.

Используя логарифмическое преобразование, вычисление апостериорной вероятности может быть упрощено до суммы логарифмов:

$
\log P(C|X) = \log P(C) + \sum_{i=1}^n \log P(x_i|C) - \log P(X)
$

Поскольку $ P(X) $ одинаково для всех классов, при сравнении разных классов его можно игнорировать, поэтому мы сосредотачиваемся только на:

$
\log P(C|X) \propto \log P(C) + \sum_{i=1}^n \log P(x_i|C)
$

Необходимо вычислить апостериорную вероятность для каждого класса:

1.  Класс $A$ (больной)：

$
\log P(A) + \sum_{i=1}^n \log P(x_i|A)
$

2.  Класс $B$ (здоровый)：

$
\log P(B) + \sum_{i=1}^n \log P(x_i|B)
$

Выбрать класс с максимальной вероятностью в качестве предсказанного результата.